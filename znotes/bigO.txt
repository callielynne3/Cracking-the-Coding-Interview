~ BIG O ~

• Big O is a measure of the efficiency of an algorithm.

• Best Case, Worst Case, Expected Case
  • Best Case - O(N)
    • traverse through an array once
  • Worst Case - O(N^2)
    • shrinks subarray by one element at a time
  • Expected Case - O(N log N)  
    • pivot can be high or low, but it won't happen over and over again

• Time Complexity -  quantifies the amount of time taken by an algorithm to run.
• Space Complexity - a measure of the amount of working storage/memory an algorithm requires.

• Drop the Constants - it is very possible for O(N) code to run faster than 0(1) code for specific inputs. Big 0
  just describes the rate of increase. For this reason, we drop the constants in runtime. An algorithm that one
  might have described as 0 (2N) is actually O(N).

• Drop the Non-Dominant Terms - what do you do about an expression such as 0 (W + N)? That second N isn't exactly a 
  constant, but it's not especially important. We already said that we drop constants. Therefore, 0 (N2 + N2) would be 0 (N2) . If we don't care about that latter N2 term, why would we care about N? We don't. You should drop the non-dominant terms. 
    • O(W + N) becomes O(W)
    • O(N + log N) becomes O(N)
    • 0(5*2N + 1000N100 ) becomes O(2N)

• Multi-Part Algorithms: Add vs. Multiply
  • If the algorithm is in the form "do this, then, when you're all done, do that" then you add the runtimes.
    • Ex: two separate for loops, once one completes, the other starts.
  • If the algorithm is in the form "do this for each time you do that" then you multiply the runtimes.
    • Ex: nested for loops.

• Amortized Time - an ArrayList, or a dynamically resizing array, allows you to have the benefits of an array while 
  offering flexibility in size. You won't run out of space in the ArrayList since its capacity will grow as you insert elements. An ArrayList is implemented with an array. When the array hits capacity, the ArrayList class will create a new array with double the capacity and copy all the elements over to the new array. If the array contains N elements, then inserting a new element will take 0 (N) time. You will have to create a new array of size 2N and then copy N elements over. This insertion will take 0 (N) time. However, we also know that this doesn't happen very often. The vast majority of the time insertion will be in 0(1) time. We need a concept that takes both into account. This is what amortized time does. It allows us to describe that, yes, this worst case happens every once in a while. But once it happens, it won't happen again for so long that the cost is 'amortized.'

• Log N Runtimes - O( log N)
  • In binary search, we are looking for an example x in an N-element sorted array. We first compare x to the
    midpoint of the array. If x == middle, then we return. If x < middle, then we search on the left side of the array. If x > middle, then we search on the right side of the array.
      search 9 within {1, 5, 8, 9, 11, 13, 15, 19, 21}
      compare 9 to 11 -> smaller.
      search 9 within {1, 5, 8, 9, 11}
      compare 9 to 8 -> bigger
      search 9 within {9, 11}
      compare 9 to 9
      return
      We start off with an N-element array to search. Then, after a single step, we're down to ~ elements. One more step, and we're down to X elements. We stop when we either find the value or we're down to just one element. The total runtime is then a matter of how many steps (dividing N by 2 each time) we can take until N becomes 1.
      N = 16
      N = 8 /* divide by 2 */
      N = 4 /* divide by 2 */
      N = 2 /* divide by 2 */
      N = 1 /* divide by 2 */
      We could look at this in reverse (going from 1 to 16 instead of 16 to 1). How many times we can multiply 1 by 2 until we get N?
      N = 1 /* multiply by 2 */
      N = 2 /* multiply by 2 */
      N = 4 /* multiply by 2 */
      N = 8 /* multiply by 2 */
      N = 16 /* multiply by 2 */
  •  What is k in the expression 2k = N? This is exactly what log expresses. 
      24 = 16 -> log,16 = 4 log,N = k -> 2k = N 
      This is a good takeaway for you to have. When you see a problem where the number of elements in the problem space gets halved each time, that will likely be a O( log N) runtime. This is the same reason why finding an element in a balanced binary search tree is 0 (log N) . With each comparison, we go either left or right. Half the nodes are on each side, so we cut the problem space in half each time.